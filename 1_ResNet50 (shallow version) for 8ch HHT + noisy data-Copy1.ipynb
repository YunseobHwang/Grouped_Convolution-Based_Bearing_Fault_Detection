{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T10:42:40.211350Z",
     "start_time": "2019-12-13T10:42:39.697145Z"
    }
   },
   "source": [
    "__Skip Connection / Bottleneck Skip connection__\n",
    "\n",
    "![image](https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile7.uf.tistory.com%2Fimage%2F99F0453F5C47F1741338F0)\n",
    "\n",
    "- ResNet50 부터는 연산량의 줄이기 위해 Residual Block 내에, 1x1, 3x3, 1x1 컨볼루션 연산을 쌓았다. Inception에서 배웠던 것과 같은 개념이다. 1x1 컨볼루션 연산으로 피쳐맵의 갯수를 줄였다가 3x3을 거친 후, 1x1 컨볼루션 연산으로 차원을 늘려준다. 이 과정이 병목 같다 하여 병목레이어(bottleneck layer)라고 부른다.\n",
    "\n",
    "__Residual Block / Identity Block__\n",
    "\n",
    "![image](https://datascienceschool.net/upfiles/2e104ff279804e839cef46fc58ef16e7.png)\n",
    "\n",
    "-  이미지가 반으로 작아진 경우, Identity Block이 사용되며, 입력값을 바로 더하지 않고, 1x1 컨볼루션 연산을 스트라이드 2로 설정하여 피쳐맵의 크기와 갯수를 맞추어준 다음 더해준다. 이를 프로젝션 숏컷(projection shortcut)\n",
    "\n",
    "__ResNet Structrue by layer__\n",
    "\n",
    "![image](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99167C335C47F0E315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T08:35:24.094837Z",
     "start_time": "2019-12-15T08:35:22.286724Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU setting\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "# modules setting\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import datetime\n",
    "from utils import one_hot, train_valid_split, random_minibatch, shuffle\n",
    "from utils import training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T08:35:47.096615Z",
     "start_time": "2019-12-15T08:35:47.084021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ball_7.npy',\n",
       " 'ball_14.npy',\n",
       " 'ball_21.npy',\n",
       " 'inner_7.npy',\n",
       " 'inner_14.npy',\n",
       " 'inner_21.npy',\n",
       " 'outer_7.npy',\n",
       " 'outer_14.npy',\n",
       " 'outer_21.npy',\n",
       " 'normal.npy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir =  '/mnt/disk1/yunseob/courses/19-2_computer vision/data/HHT/8ch/train'\n",
    "npy_files = os.listdir(train_dir)\n",
    "npy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T08:35:51.972506Z",
     "start_time": "2019-12-15T08:35:48.385854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal: (750, 100, 100, 8) (750, 10)\n",
      "ball_7: (750, 100, 100, 8) (750, 10)\n",
      "ball_14: (750, 100, 100, 8) (750, 10)\n",
      "ball_21: (750, 100, 100, 8) (750, 10)\n",
      "inner_7: (750, 100, 100, 8) (750, 10)\n",
      "inner_14: (750, 100, 100, 8) (750, 10)\n",
      "inner_21: (750, 100, 100, 8) (750, 10)\n",
      "outer_7: (750, 100, 100, 8) (750, 10)\n",
      "outer_14: (750, 100, 100, 8) (750, 10)\n",
      "outer_21: (750, 100, 100, 8) (750, 10)\n"
     ]
    }
   ],
   "source": [
    "normal = np.load(os.path.join(train_dir, str([i for i in npy_files if 'normal' in i][0])))\n",
    "ball_7 = np.load(os.path.join(train_dir, str([i for i in npy_files if 'ball_7' in i][0])))\n",
    "ball_14 = np.load(os.path.join(train_dir, str([i for i in npy_files if 'ball_14' in i][0])))\n",
    "ball_21 = np.load(os.path.join(train_dir, str([i for i in npy_files if 'ball_21' in i][0])))\n",
    "inner_7 = np.load(os.path.join(train_dir, str([i for i in npy_files if 'inner_7' in i][0])))\n",
    "inner_14 = np.load(os.path.join(train_dir, str([i for i in npy_files if 'inner_14' in i][0])))\n",
    "inner_21 = np.load(os.path.join(train_dir, str([i for i in npy_files if 'inner_21' in i][0])))\n",
    "outer_7 = np.load(os.path.join(train_dir, str([i for i in npy_files if 'outer_7' in i][0])))\n",
    "outer_14 = np.load(os.path.join(train_dir, str([i for i in npy_files if 'outer_14' in i][0])))\n",
    "outer_21 = np.load(os.path.join(train_dir, str([i for i in npy_files if 'outer_21' in i][0])))\n",
    "\n",
    "normal_y = one_hot(normal, 0, nb_classes = 10)\n",
    "ball_7_y = one_hot(ball_7, 1, nb_classes = 10)\n",
    "ball_14_y = one_hot(ball_14, 2, nb_classes = 10)\n",
    "ball_21_y = one_hot(ball_21, 3, nb_classes = 10)\n",
    "inner_7_y = one_hot(inner_7, 4, nb_classes = 10)\n",
    "inner_14_y = one_hot(inner_14, 5, nb_classes = 10)\n",
    "inner_21_y = one_hot(inner_21, 6, nb_classes = 10)\n",
    "outer_7_y = one_hot(outer_7, 7, nb_classes = 10)\n",
    "outer_14_y = one_hot(outer_14, 8, nb_classes = 10)\n",
    "outer_21_y = one_hot(outer_21, 9, nb_classes = 10)\n",
    "\n",
    "print(\"normal:\", normal.shape, normal_y.shape)\n",
    "print(\"ball_7:\", ball_7.shape, ball_7_y.shape)\n",
    "print(\"ball_14:\", ball_14.shape, ball_14_y.shape)\n",
    "print(\"ball_21:\", ball_21.shape, ball_21_y.shape)\n",
    "print(\"inner_7:\", inner_7.shape, inner_7_y.shape)\n",
    "print(\"inner_14:\", inner_14.shape, inner_14_y.shape)\n",
    "print(\"inner_21:\", inner_21.shape, inner_21_y.shape)\n",
    "print(\"outer_7:\", outer_7.shape, outer_7_y.shape)\n",
    "print(\"outer_14:\", outer_14.shape, outer_14_y.shape)\n",
    "print(\"outer_21:\", outer_21.shape, outer_21_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T08:37:02.692554Z",
     "start_time": "2019-12-15T08:36:58.092274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal: (638, 100, 100, 8) (638, 10) (112, 100, 100, 8) (112, 10)\n",
      "ball_7: (638, 100, 100, 8) (638, 10) (112, 100, 100, 8) (112, 10)\n",
      "ball_14: (638, 100, 100, 8) (638, 10) (112, 100, 100, 8) (112, 10)\n",
      "ball_21: (638, 100, 100, 8) (638, 10) (112, 100, 100, 8) (112, 10)\n",
      "inner_7: (638, 100, 100, 8) (638, 10) (112, 100, 100, 8) (112, 10)\n",
      "inner_14: (638, 100, 100, 8) (638, 10) (112, 100, 100, 8) (112, 10)\n",
      "inner_21: (638, 100, 100, 8) (638, 10) (112, 100, 100, 8) (112, 10)\n",
      "outer_7: (638, 100, 100, 8) (638, 10) (112, 100, 100, 8) (112, 10)\n",
      "outer_14: (638, 100, 100, 8) (638, 10) (112, 100, 100, 8) (112, 10)\n",
      "outer_21: (638, 100, 100, 8) (638, 10) (112, 100, 100, 8) (112, 10)\n"
     ]
    }
   ],
   "source": [
    "normal_train_x, normal_train_y, normal_valid_x, normal_valid_y = train_valid_split(normal, normal_y)\n",
    "print(\"normal:\", normal_train_x.shape, normal_train_y.shape, normal_valid_x.shape, normal_valid_y.shape)\n",
    "\n",
    "ball_7_train_x, ball_7_train_y, ball_7_valid_x, ball_7_valid_y = train_valid_split(ball_7, ball_7_y)\n",
    "ball_14_train_x, ball_14_train_y, ball_14_valid_x, ball_14_valid_y = train_valid_split(ball_14, ball_14_y)\n",
    "ball_21_train_x, ball_21_train_y, ball_21_valid_x, ball_21_valid_y = train_valid_split(ball_21, ball_21_y)\n",
    "print(\"ball_7:\", ball_7_train_x.shape, ball_7_train_y.shape, ball_7_valid_x.shape, ball_7_valid_y.shape)\n",
    "print(\"ball_14:\", ball_14_train_x.shape, ball_14_train_y.shape, ball_14_valid_x.shape, ball_14_valid_y.shape)\n",
    "print(\"ball_21:\", ball_21_train_x.shape, ball_21_train_y.shape, ball_21_valid_x.shape, ball_21_valid_y.shape)\n",
    "\n",
    "inner_7_train_x, inner_7_train_y, inner_7_valid_x, inner_7_valid_y = train_valid_split(inner_7, inner_7_y)\n",
    "inner_14_train_x, inner_14_train_y, inner_14_valid_x, inner_14_valid_y = train_valid_split(inner_14, inner_14_y)\n",
    "inner_21_train_x, inner_21_train_y, inner_21_valid_x, inner_21_valid_y = train_valid_split(inner_21, inner_21_y)\n",
    "print(\"inner_7:\", inner_7_train_x.shape, inner_7_train_y.shape, inner_7_valid_x.shape, inner_7_valid_y.shape)\n",
    "print(\"inner_14:\", inner_14_train_x.shape, inner_14_train_y.shape, inner_14_valid_x.shape, inner_14_valid_y.shape)\n",
    "print(\"inner_21:\", inner_21_train_x.shape, inner_21_train_y.shape, inner_21_valid_x.shape, inner_21_valid_y.shape)\n",
    "\n",
    "outer_7_train_x, outer_7_train_y, outer_7_valid_x, outer_7_valid_y = train_valid_split(outer_7, outer_7_y)\n",
    "outer_14_train_x, outer_14_train_y, outer_14_valid_x, outer_14_valid_y = train_valid_split(outer_14, outer_14_y)\n",
    "outer_21_train_x, outer_21_train_y, outer_21_valid_x, outer_21_valid_y = train_valid_split(outer_21, outer_21_y)\n",
    "print(\"outer_7:\", outer_7_train_x.shape, outer_7_train_y.shape, outer_7_valid_x.shape, outer_7_valid_y.shape)\n",
    "print(\"outer_14:\", outer_14_train_x.shape, outer_14_train_y.shape, outer_14_valid_x.shape, outer_14_valid_y.shape)\n",
    "print(\"outer_21:\", outer_21_train_x.shape, outer_21_train_y.shape, outer_21_valid_x.shape, outer_21_valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T08:37:10.198981Z",
     "start_time": "2019-12-15T08:37:05.123907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (6380, 100, 100, 8) (6380, 10)\n",
      "Validation set: (1120, 100, 100, 8) (1120, 10)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.vstack([normal_train_x, ball_7_train_x, ball_14_train_x, ball_21_train_x, \n",
    "                     inner_7_train_x, inner_14_train_x, inner_21_train_x,\n",
    "                     outer_7_train_x, outer_14_train_x, outer_21_train_x, ])\n",
    "train_Y = np.vstack([normal_train_y, ball_7_train_y, ball_14_train_y, ball_21_train_y, \n",
    "                     inner_7_train_y, inner_14_train_y, inner_21_train_y,\n",
    "                     outer_7_train_y, outer_14_train_y, outer_21_train_y, ])\n",
    "valid_X = np.vstack([normal_valid_x, ball_7_valid_x, ball_14_valid_x, ball_21_valid_x, \n",
    "                     inner_7_valid_x, inner_14_valid_x, inner_21_valid_x,\n",
    "                     outer_7_valid_x, outer_14_valid_x, outer_21_valid_x, ])\n",
    "valid_Y = np.vstack([normal_valid_y, ball_7_valid_y, ball_14_valid_y, ball_21_valid_y, \n",
    "                     inner_7_valid_y, inner_14_valid_y, inner_21_valid_y,\n",
    "                     outer_7_valid_y, outer_14_valid_y, outer_21_valid_y, ])\n",
    "\n",
    "print(\"Training set:\", train_X.shape, train_Y.shape)\n",
    "print(\"Validation set:\", valid_X.shape, valid_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T08:39:33.397098Z",
     "start_time": "2019-12-15T08:39:33.384614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['minus4.npy',\n",
       " 'minus4_y.npy',\n",
       " 'minus2.npy',\n",
       " 'minus2_y.npy',\n",
       " '0.npy',\n",
       " '0_y.npy',\n",
       " '2.npy',\n",
       " '2_y.npy',\n",
       " '4.npy',\n",
       " '4_y.npy',\n",
       " '6.npy',\n",
       " '6_y.npy',\n",
       " '8.npy',\n",
       " '8_y.npy',\n",
       " '10.npy',\n",
       " '10_y.npy']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_dir =  '/mnt/disk1/yunseob/courses/19-2_computer vision/data/HHT/8ch_noisy/train'\n",
    "npy_files = os.listdir(noise_dir)\n",
    "npy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T08:39:34.658105Z",
     "start_time": "2019-12-15T08:39:34.336111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 100, 100, 8) (800, 10)\n"
     ]
    }
   ],
   "source": [
    "noise_10_x, noise_10_y = np.load(noise_dir + '/10.npy'), np.load(noise_dir + '/10_y.npy')\n",
    "\n",
    "print(noise_10_x.shape, noise_10_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T08:44:24.292204Z",
     "start_time": "2019-12-15T08:44:23.890323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise data\n",
      "Training set: (680, 100, 100, 8) (680, 10)\n",
      "Validation set: (120, 100, 100, 8) (120, 10)\n"
     ]
    }
   ],
   "source": [
    "train_noise_X, train_noise_Y, valid_noise_X, valid_noise_Y = train_valid_split(noise_10_x, noise_10_y)\n",
    "\n",
    "print(\"Noise data\")\n",
    "print(\"Training set:\", train_noise_X.shape, train_noise_Y.shape)\n",
    "print(\"Validation set:\", valid_noise_X.shape, valid_noise_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T08:45:46.749969Z",
     "start_time": "2019-12-15T08:45:46.377348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 100, 100, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((train_noise_X, valid_noise_X), axis = 0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T08:48:00.440184Z",
     "start_time": "2019-12-15T08:48:00.432257Z"
    }
   },
   "outputs": [],
   "source": [
    "input_h = 100\n",
    "input_w = 100\n",
    "input_ch = 8\n",
    "\n",
    "ch = 16\n",
    "# 50 50 16\n",
    "\n",
    "r_ch_1 = 32\n",
    "# 25 25 32\n",
    "\n",
    "r_ch_2 = 32\n",
    "# 12 12 16\n",
    "\n",
    "r_ch_3 = 64\n",
    "# 12 12 32\n",
    "\n",
    "r_ch_4 = 128\n",
    "# 6 6 128\n",
    "\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T08:48:04.076434Z",
     "start_time": "2019-12-15T08:48:01.123797Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, input_h, input_w, input_ch], name = 'img')\n",
    "y = tf.placeholder(tf.float32, [None, n_output], name = 'label')\n",
    "batch_prob = tf.placeholder(tf.bool, name = 'bn_prob')\n",
    "\n",
    "class ResNet50:\n",
    "    def __init__(self, ch, r_ch_1, r_ch_2, r_ch_3, r_ch_4):\n",
    "        self.ch = ch\n",
    "        self.r_ch_1 = r_ch_1\n",
    "        self.r_ch_2 = r_ch_2\n",
    "        self.r_ch_3 = r_ch_3\n",
    "        self.r_ch_4 = r_ch_4\n",
    "    def conv(self, x, channel, kernel_size = [3, 3], strides = (1, 1), activation = True):\n",
    "        conv = tf.layers.conv2d(inputs = x, filters = channel, kernel_size = kernel_size, \n",
    "                                strides = strides, padding = \"SAME\")\n",
    "        conv = tf.layers.batch_normalization(conv, center=True, scale=True, training=batch_prob)\n",
    "        if activation == True:\n",
    "            conv = tf.nn.relu(conv)\n",
    "        return conv\n",
    "    \n",
    "    def maxp(self, conv):\n",
    "        maxp = tf.layers.max_pooling2d(inputs = conv, pool_size = [2, 2], strides = 2)\n",
    "        return maxp\n",
    "\n",
    "    def res_block(self, x, channel):\n",
    "        x_shortcut = x\n",
    "        conv_a = self.conv(x, channel/4, kernel_size = [1, 1])\n",
    "        conv_b = self.conv(conv_a, channel/4, kernel_size = [3, 3])\n",
    "        conv_c = self.conv(conv_b, channel, kernel_size = [1, 1])\n",
    "        return tf.nn.relu(conv_c + x_shortcut)\n",
    "    \n",
    "    def id_block(self, x, channel, strides = (2, 2)):\n",
    "        x_shortcut = x\n",
    "        conv_2a = self.conv(x, channel/4, kernel_size = [1, 1], strides = strides)\n",
    "        conv_2b = self.conv(conv_2a, channel/4, kernel_size = [3, 3])\n",
    "        conv_2c = self.conv(conv_2b, channel, kernel_size = [1, 1], activation = False)\n",
    "        conv_1 = self.conv(x, channel, kernel_size = [1, 1], strides = strides, activation = False)\n",
    "        return tf.nn.relu(conv_1 + conv_2c)\n",
    "\n",
    "    def fc_layer(self, gap, n_output = None):\n",
    "        flatten = tf.layers.flatten(gap)\n",
    "        output = tf.layers.dense(inputs = flatten, units = n_output)\n",
    "        return output\n",
    "\n",
    "    def global_avg_pooling(self, x):\n",
    "        gap = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
    "        return gap\n",
    "\n",
    "    def res_block_rep(self, x, target_ch, n_rep):\n",
    "        for _ in range(n_rep):\n",
    "            x = self.res_block(x, target_ch)\n",
    "        return x\n",
    "\n",
    "    def inf(self, x):\n",
    "        \"\"\"\n",
    "        conv_1: 1\n",
    "        id_~ + resnet_~: 16 x 3 = 48\n",
    "        fc_lay: 1\n",
    "\n",
    "        total: 50\n",
    "        \"\"\"\n",
    "        conv_1 = self.conv(x, self.ch, strides = (2, 2))\n",
    "        maxp_1 = self.maxp(conv_1)\n",
    "        id_1 = self.id_block(maxp_1, self.r_ch_1, strides = (1, 1))\n",
    "        res_1 = self.res_block_rep(id_1, self.r_ch_1, n_rep = 2)\n",
    "        id_2 = self.id_block(res_1,self.r_ch_2, strides = (2, 2))\n",
    "        res_2 = self.res_block_rep(id_2, self.r_ch_2, n_rep = 3)\n",
    "        id_3 = self.id_block(res_2, self.r_ch_3, strides = (2, 2))\n",
    "        res_3 = self.res_block_rep(id_3, self.r_ch_3, n_rep = 5)\n",
    "        id_4 = self.id_block(res_3, self.r_ch_4, strides = (2, 2))\n",
    "        res_4 = self.res_block_rep(id_4, self.r_ch_4, n_rep = 2)\n",
    "        gap = self.global_avg_pooling(res_4)\n",
    "        score = self.fc_layer(gap, n_output)\n",
    "        return score\n",
    "\n",
    "    \n",
    "model = ResNet50(ch, r_ch_1, r_ch_2, r_ch_3, r_ch_4)\n",
    "score = model.inf(x)\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=score)\n",
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-15T08:48:03.794Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_batch = 8\n",
    "v_batch = 64\n",
    "n_cal = 10\n",
    "n_prt = 100\n",
    "\n",
    "n_iter = 0\n",
    "\n",
    "# LR = 1e-4 # 1e-4 ~ 5e-4 (xavier)\n",
    "lr = 1e-4\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optm = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "# optm = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "start_time = time.time() \n",
    "\n",
    "accr_train, accr_valid, loss_train, loss_valid = [], [], [], []\n",
    "early_stopping = False\n",
    "\n",
    "hist = training_history(accr_train, accr_valid, loss_train, loss_valid)\n",
    "hist.table()\n",
    "\n",
    "while True:\n",
    "    train_x, train_y = random_minibatch(train_X, train_Y, batch_size = t_batch)\n",
    "    train_n_x, train_n_y = random_minibatch(train_noise_X, train_noise_Y, batch_size = t_batch)\n",
    "    \n",
    "    train_x, train_y = np.concatenate((train_x, train_n_x), axis = 0), np.concatenate((train_y, train_n_y), axis = 0)\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    \n",
    "    sess.run(optm, feed_dict = {'img:0': train_x, 'label:0': train_y, 'bn_prob:0' :1})\n",
    "    n_iter += 1\n",
    "    if n_iter % n_cal == 0:\n",
    "        c, p = sess.run([loss, score], feed_dict = {'img:0': train_x, 'label:0': train_y, 'bn_prob:0' :0})\n",
    "\n",
    "        p = np.argmax(p, axis = 1)\n",
    "        l = np.argmax(train_y, axis = 1)\n",
    "        a = np.mean(np.equal(p, l))\n",
    "        \n",
    "        valid_x, valid_y = random_minibatch(valid_X, valid_Y, batch_size = v_batch)\n",
    "        valid_n_x, valid_n_y = random_minibatch(valid_noise_X, valid_noise_Y, batch_size = v_batch)\n",
    "        valid_x, valid_y = np.concatenate((valid_x, valid_n_x), axis = 0), np.concatenate((valid_y, valid_n_y), axis = 0)\n",
    "        \n",
    "        c_valid, p_valid = sess.run([loss, score], feed_dict = {'img:0': valid_x, 'label:0': valid_y, 'bn_prob:0' :0})\n",
    "\n",
    "        p_valid = np.argmax(p_valid, axis = 1)\n",
    "        l_valid = np.argmax(valid_y, axis = 1)\n",
    "        a_valid = np.mean(np.equal(p_valid, l_valid))\n",
    "\n",
    "        accr_valid.append(a_valid)\n",
    "        loss_valid.append(c_valid)\n",
    "        accr_train.append(a)\n",
    "        loss_train.append(c)\n",
    "\n",
    "        if n_iter % n_prt == 0:\n",
    "            hist.prt_evl(n_iter)\n",
    "\n",
    "        if loss_valid[-1] == np.min(loss_valid):\n",
    "            now = datetime.datetime.now()\n",
    "            nowDatetime = now.strftime('%y%m%d%H%M')\n",
    "            model_name = 'hht_8ch_n10_res50_{0}_{1}_val_acc_{2:.2f}_val_loss_{3:.6f}'.format(nowDatetime, n_iter, accr_valid[-1], loss_valid[-1])\n",
    "            saver.save(sess, './model/HHT/' + model_name)\n",
    "        if n_iter == 40000:\n",
    "            break\n",
    "#         if n_iter > 1000:\n",
    "#             if np.max(accr_train) < 0.9:\n",
    "#                 if np.mean(loss_train[-50:-30]) <= np.mean(loss_train[-30:]) :\n",
    "#                     hist.early_under(n_iter)\n",
    "#                     early_stopping = True\n",
    "#                     break\n",
    "#             if np.mean(accr_train[-50:]) >= 0.995:\n",
    "#                 if (\n",
    "#                     np.mean(loss_valid[-41:-21]) <= np.mean(loss_valid[-21:-1]) and\n",
    "#                     loss_valid[-1] < loss_valid[-2] # np.min(loss_valid[-20:]) == loss_valid[-1]\n",
    "#                     ):\n",
    "#                     hist.early_over(n_iter)\n",
    "#                     early_stopping = True\n",
    "#                     break          \n",
    "\n",
    "train_time = int((time.time() - start_time)/60)  \n",
    "hist.done(n_iter, train_time, early_stopping)\n",
    "\n",
    "np.save('/mnt/disk1/yunseob/courses/19-2_computer vision/history/ResNet50_HHT_8ch_n10_accr', np.array(accr_train))\n",
    "np.save('/mnt/disk1/yunseob/courses/19-2_computer vision/history/ResNet50_HHT_8ch_n10_loss', np.array(loss_train))\n",
    "\n",
    "hist.plot(n_cal)   \n",
    "\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
